{"cells":[{"cell_type":"markdown","metadata":{"id":"-4OUvVfQLgR4"},"source":["\n","XLM-Roberta on multilingual text\n","\n","- Training data + augmentation data\n","- Weighted loss = 50% Overall + (50% / 6) for each of the remaining 6 labels\n","- Model parameter file: XLM_Roberta_0.50_overall_loss2.pth\n","- Using dropout in the linear layers"]},{"cell_type":"markdown","metadata":{"id":"Bhj8tK3WESKl"},"source":["0. Install and import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23425,"status":"ok","timestamp":1654052587309,"user":{"displayName":"REEMA KUMARI","userId":"05077737033500745545"},"user_tz":420},"id":"Q-Mlm41yMmLS","outputId":"9aa8bd5b-bcdf-4fe5-fa2d-b44fef450975"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 14.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 63.4 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 6.3 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 56.1 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 5.0 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"]}],"source":["## install libraries\n","\n","!pip install transformers\n","!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lH_HtKSJLeWh"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random\n","import json\n","import regex as re\n","\n","import matplotlib.pyplot as plt\n","import os\n","import sentencepiece\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer\n","from transformers import AdamW\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader\n","from transformers import XLMRobertaConfig, XLMRobertaModel, XLMRobertaTokenizer\n","\n","torch.cuda.empty_cache()\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","LIBRARY_PATH = '/content/drive/MyDrive/NLP PROJECT/Finals/'\n","\n","# Seed\n","seed = 123\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{"id":"RFsYbEgFEXYX"},"source":["## 1. Read and prepare data for training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lSH-HnE-fuT7"},"outputs":[],"source":["def clean_text(text, fixed_length = True, length = 256, head_len = 200, tail_len = 56):\n","  text = re.sub(r'http\\S+', '', text)\n","  text = re.sub(\"\\n|\\r\", \" \", text)\n","  text = re.sub(\"['']\", \"\", text)\n","\n","  if(fixed_length == True):\n","    tokens = text.split()\n","    if(len(tokens) > length):\n","      head = tokens[:head_len]\n","      tail = tokens[-tail_len:]\n","      text = ' '.join(head+tail)\n","\n","  return text\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z_eC8fRVL4FV"},"outputs":[],"source":["raw_data_path = LIBRARY_PATH + 'data/processed/'\n","raw_data_filename = 'train_DA.csv'\n","train_data = pd.read_csv(raw_data_path + raw_data_filename)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WkEpEc0p4qI7"},"outputs":[],"source":["## EDA \n","\n","## Check distribution of missing values\n","train_data.head()\n","eda_data1 = train_data[['link_id1', 'title1', 'text1', 'meta_keywords1', 'meta_description1']].drop_duplicates()\n","eda_data1.columns = ['ids', 'title', 'text', 'meta_keywords', 'meta_description']\n","eda_data2 = train_data[['link_id2', 'title2', 'text2', 'meta_keywords2', 'meta_description2']].drop_duplicates()\n","eda_data2.columns = ['ids', 'title', 'text', 'meta_keywords', 'meta_description']\n","\n","eda_df = pd.concat([eda_data1, eda_data2])\n","\n","## Print missing or blanks\n","# for meta_keywords\n","tmp = eda_df[ ~ ((eda_df['meta_keywords'] == '['']') | (eda_df['meta_keywords'] == ''))]\n","print(tmp['meta_keywords'].value_counts())\n","# [''], ''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3tMHzaz-H7Hp"},"outputs":[],"source":["## Split into train and eval\n","\n","# remove na text from both the text fields\n","processed_data = train_data[train_data['text1'].notna()]\n","processed_data = processed_data[processed_data['text2'].notna()]\n","\n","print(\"After removing NA text columns, we lose {0} rows\".format(train_data.shape[0] - processed_data.shape[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V1JWq5jymzwr"},"outputs":[],"source":["def merge_clean_columns(df):\n","    \"\"\"\n","    Merge multiple columns into one and clean text\n","    \"\"\"  \n","    df['merge1'] = df['meta_keywords1'].astype(str) + ', ' \\\n","        + df['meta_description1'].astype(str) + ', '\\\n","        + df['title1'].astype(str) + ', '\\\n","        + df['text1'].astype(str)\n","\n","    df['merge2'] = df['meta_keywords2'].astype(str) + ', ' \\\n","        + df['meta_description2'].astype(str) + ', '\\\n","        + df['title2'].astype(str) + ', '\\\n","        + df['text2'].astype(str)\n","\n","    df['merge1'] = df['merge1'].apply(lambda x: clean_text(x))\n","    df['merge2'] = df['merge2'].apply(lambda x: clean_text(x))\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KNWWDBCcmwSh"},"outputs":[],"source":["processed_data = merge_clean_columns(processed_data)\n","# split into train and development\n","train, dev = train_test_split(processed_data, test_size=0.1, random_state = 42)"]},{"cell_type":"markdown","metadata":{"id":"tZMXmR0_QNzQ"},"source":["## 2. Model on data text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9NYsKPM8QKjF"},"outputs":[],"source":["## set parameters\n","max_len = 512\n","batch_size = 5\n","lr = 5e-6\n","weight_decay = 1e-4\n","num_epochs = 8"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LM33gaFg7rRE"},"outputs":[],"source":["def get_data_loader(data, batch_size_flg = True):\n","  tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n","  input_ids, attention_masks, labels = [], [], []\n","  for idx, row in data.iterrows():\n","      text1, text2 = row['merge1'], row['merge2']\n","      encode_dict = tokenizer(text1,text2,\n","                                  max_length=max_len,\n","                                  padding='max_length',\n","                                  truncation=True,\n","                                  add_special_tokens=True\n","                                  )\n","      \n","      input_ids.append(encode_dict['input_ids'])\n","      attention_masks.append(encode_dict['attention_mask'])\n","      # model is used to predict all labels?? -> should we convert to only 1 label\n","      labels.append([float(x) for x in [row['Geography'],row['Entities'],row['Time'],row['Narrative'],row['Overall'],row['Style'],row['Tone']]])\n","\n","  input_ids = torch.tensor(input_ids)\n","  attention_masks = torch.tensor(attention_masks)\n","  labels = torch.tensor(labels)\n","\n","  data = TensorDataset(input_ids, attention_masks, labels)\n","  if(batch_size_flg):\n","      data_loader = DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n","  else:\n","      data_loader = DataLoader(data)\n","  return data_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vg56V9t08atc"},"outputs":[],"source":["train_data_loader = get_data_loader(train)\n","eval_data_loader = get_data_loader(dev, False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rTDTaFQmsA8O"},"outputs":[],"source":["class Custom_XLMRoberta(nn.Module):\n","    def __init__(self, model, hidden_size):\n","        super(Custom_XLMRoberta, self).__init__()\n","        self.reg_model = model\n","        self.fc1 = nn.Linear(hidden_size, 100)\n","        self.dropout = nn.Dropout(0.2)\n","        self.fc2 = nn.Linear(100, 7) ## currently processes the 7 labels that we have defined for 7 output types\n","        self.activation = nn.GELU()\n","\n","    def forward(self, input_ids, attention_masks):\n","        output1 = self.reg_model(input_ids, attention_masks)[1]\n","        output1 = self.dropout(output1)\n","        output2 = self.activation(self.fc1(output1))\n","        output2 = self.dropout(output2)\n","        # x = self.dropout(x)\n","        logits1 = self.fc2(output2)\n","        return logits1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWuuxWC9wvSs"},"outputs":[],"source":["\n","def predict(model, data_loader):\n","  model.eval()\n","  overall_pred, overall_true = [], []\n","  with torch.no_grad():\n","    for idx, (ids, att_msks, y) in enumerate(data_loader):\n","      ids, att_msks, y = ids.to(device), att_msks.to(device), y.to(device)\n","      y_pred = model(ids, att_msks)\n","      y_pred, y = torch.squeeze(y_pred).cpu().numpy().tolist(), torch.squeeze(y).cpu().numpy().tolist()\n","      overall_pred.append(y_pred[4])\n","      overall_true.append(y[4])\n","  return overall_pred, overall_true\n","\n","\n","def weighted_loss( y_pred, y, criterion, loss_weights):\n","  loss = 0.0\n","  for i in range(7):\n","    y_pred_i, y_i = y_pred[:, i], y[:, i]\n","    loss += criterion(y_pred_i, y_i) * loss_weights[i]\n","  return loss\n","\n","\n","def train(model, model_path, train_data_loader, eval_data_loader, optimizer, loss_weights, epochs):\n","  model.train()\n","  criterion = nn.MSELoss()\n","  best_pearson = 0\n","  for i in range(epochs):\n","    train_loss_sum = 0\n","    for idx, (ids, att_msks, y) in enumerate(train_data_loader):\n","      ids, att_msks, y = ids.to(device), att_msks.to(device), y.to(device)\n","      optimizer.zero_grad()\n","      y_pred = model(ids, att_msks)\n","      y_pred, y = torch.squeeze(y_pred), torch.squeeze(y) ## required because y is a vector\n","      loss = weighted_loss(y_pred, y, criterion, loss_weights)\n","      loss.backward()\n","      optimizer.step()\n","      train_loss_sum += loss.item()\n","\n","    print(f\"Loss at epoch {i}: {train_loss_sum:.4f}\")\n","\n","    ## Determine best epoch model using correlation coefficient for Overall in dev data\n","    eval_pred_overall, eval_true_overall = predict(model, eval_data_loader)\n","    curr_pearson = np.corrcoef(eval_pred_overall, eval_true_overall)[0][1]\n","    print(curr_pearson)\n","    if curr_pearson > best_pearson:\n","      best_pearson = curr_pearson\n","      torch.save(model.state_dict(), model_path)\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"loEOryJU53q8"},"source":["### Training model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"referenced_widgets":["3baf3f9e40a04c0a97082ed813428095"]},"id":"5ldBSUrA68uK","outputId":"1a85f722-8d7e-40ee-9e09-5d6ca811f91a"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3baf3f9e40a04c0a97082ed813428095","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Model name for this run: XLM_Roberta_base_iter_3.pth\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 0: 2630.7116\n","0.6511325413153268\n","Loss at epoch 1: 1006.6751\n","0.8188878434772799\n","Loss at epoch 2: 791.1312\n","0.8436809208111284\n","Loss at epoch 3: 667.8760\n","0.8701343897785813\n","Loss at epoch 4: 572.2973\n","0.8696804110534562\n","Loss at epoch 5: 497.6126\n","0.8686041450238611\n","Loss at epoch 6: 451.4908\n","0.8826879575891096\n","Loss at epoch 7: 413.4918\n","0.8852396722452124\n"]}],"source":["## run model finetuning and save fine-tuned model\n","# pre_trained_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-large\")\n","torch.cuda.empty_cache()\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","pre_trained_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n","config = XLMRobertaConfig.from_pretrained(\"xlm-roberta-base\")\n","hidden_size = config.hidden_size\n","# hidden_size = 768\n","\n","loss_weights = [0.5 if i == 4 else (1-0.5)/6 for i in range(7)]\n","\n","model = Custom_XLMRoberta(pre_trained_model, hidden_size)\n","model.to(device)\n","\n","\n","for iter in range(1,1000):\n","    model_name = 'XLM_Roberta_base'\n","    model_name_iter = f\"{model_name}_iter_{iter}.pth\"\n","    model_path = f\"/content/drive/MyDrive/NLP PROJECT/Finals/ModelParams/{model_name_iter}\"\n","    if not os.path.exists(model_path):\n","      break\n","\n","print(f\"Model name for this run: {model_name_iter}\")\n","\n","optimizer = AdamW(model.parameters(), lr=lr, weight_decay = weight_decay)\n","train(model, model_path, train_data_loader, eval_data_loader, optimizer, loss_weights, num_epochs)\n"]},{"cell_type":"markdown","metadata":{"id":"oTEe9rwjTP-e"},"source":["## 4. Evaluation on test data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePvc8NygTyVN"},"outputs":[],"source":["#### Get data\n","data_path = LIBRARY_PATH + 'data/processed/'\n","filename = 'paired_eval.csv'\n","path = data_path + filename\n","tmp_file = pd.read_csv(path)\n","\n","tmp_file_new = tmp_file.drop_duplicates()\n","# Drop na for text 1 and text 2\n","test_dropna_text1 = tmp_file_new[tmp_file_new['text1'].notna()]\n","test_dropna_text1_2 = test_dropna_text1[test_dropna_text1['text2'].notna()]\n","# Merge data\n","processed_test_data = merge_clean_columns(test_dropna_text1_2)\n","\n","processed_test_data = processed_test_data.rename(columns = {'GEO': 'Geography', \\\n","                                                            'ENT': 'Entities', \\\n","                                                            'TIME': 'Time', \\\n","                                                            'NAR': 'Narrative', \\\n","                                                            'STYLE': 'Style', \\\n","                                                            'TONE': 'Tone'})\n","\n","\n","test_data_loader = get_data_loader(processed_test_data, False)\n","# row['Geography'],row['Entities'],row['Time'],row['Narrative'],row['Overall'],row['Style'],row['Tone']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-uWOFA8TPRr"},"outputs":[],"source":["config = XLMRobertaConfig.from_pretrained(\"xlm-roberta-base\")\n","hidden_size = config.hidden_size\n","\n","pre_trained_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n","model = Custom_XLMRoberta(pre_trained_model, hidden_size)\n","#model.load_state_dict(torch.load(model_path))\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/NLP PROJECT/Finals/ModelParams/XLM_Roberta_0.50_overall_loss2.pth\"), strict=False)\n","model.to(device)\n","\n","test_pred_overall, test_true_overall = predict(model, test_data_loader)\n","test_pearson_score = np.corrcoef(test_pred_overall, test_true_overall)[0][1]\n","\n","print(\"Pearson score on test dataset is {:.3f}\".format(test_pearson_score))\n","\n","train_all = get_data_loader(processed_data, False)\n","train_pred_overall, train_true_overall = predict(model, train_all)\n","train_pearson_score = np.corrcoef(train_pred_overall, train_true_overall)[0][1]\n","print(\"Pearson score on entire train dataset is {:.3f}\".format(train_pearson_score))\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"XLM-Roberta_v7.ipynb","provenance":[{"file_id":"1L-sHyxzU5mrkxadwMo2Pq8pSJOI-ymjl","timestamp":1653882909211},{"file_id":"11toR0uH7FiRrpKNFUdgQLc64pfC0WLKE","timestamp":1653794689739},{"file_id":"1EmE0K3fw37J0OB25YZPXJ6uDfZC4ZYLY","timestamp":1653433187544},{"file_id":"1Pq3JIgyEfq2Jv4I06UDTGVzg9EsCqh3K","timestamp":1653427277379}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}